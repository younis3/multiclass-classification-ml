{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train multiclass classification models\n",
    "#@created by: younis3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import (Activation, BatchNormalization, Conv2D, Dense,\n",
    "                          Dropout, Flatten, MaxPooling2D)\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.3\n"
     ]
    }
   ],
   "source": [
    "print(tfjs.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH=224\n",
    "IMG_HEIGHT=224\n",
    "img_folder=r'C:/Users/younis3/Desktop/V6/Model/train/train_shapes/seg_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define constants\n",
    "IMG_HEIGHT = 80\n",
    "IMG_WIDTH  = 80\n",
    "CHANNELS   = 3\n",
    "batch_size = 8\n",
    "\n",
    "#train_path = 'C:/Users/younis3/Desktop/train_intel/archive/seg_train/seg_train/'\n",
    "#test_path  = 'C:/Users/younis3/Desktop/train_intel/archive/seg_test/seg_test/'\n",
    "#class_names = ['cat', 'chicken', 'duck', 'sheep', 'k','kk']\n",
    "\n",
    "train_path = 'C:/Users/younis3/Desktop/V6/Model/train/train_shapes/seg_train/'\n",
    "test_path  = 'C:/Users/younis3/Desktop/V6/Model/train/train_shapes/seg_test/'\n",
    "#class_names = ['bird', 'bull', 'cat', 'chicken', 'cow', 'dog', 'duck', 'elephant', 'fox', 'horse', 'kangaroo', 'lion', 'penguin', 'sheep']\n",
    "#class_names = ['banana', 'berry', 'cherry', 'grapes', 'kiwi', 'lemon', 'orange', 'pear', 'pineapple', 'watermelon']\n",
    "#class_names = ['black', 'blue', 'green', 'orange', 'pink', 'purple', 'red', 'white', 'yellow']\n",
    "class_names = ['circle', 'diamond', 'heart', 'square', 'star', 'triangle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 346 images belonging to 6 classes.\n",
      "Found 36 images belonging to 6 classes.\n",
      "Found 90 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "## Generators ##\n",
    " \n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Training\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Validation\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 78, 78, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 78, 78, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 78, 78, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 39, 39, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 37, 37, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 37, 37, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 17, 17, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 17, 17, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 17, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 128)         36992     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 69,574\n",
      "Trainable params: 69,126\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(2,2, padding='same'))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(2,2, padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(2,2, padding='same'))\n",
    "\n",
    "model.add(Conv2D(128, (3,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(2,2, padding='same'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# lr_redux = ReduceLROnPlateau(monitor='val_loss',\n",
    "#                             patience = 3, verbose = 1,\n",
    "#                             factor = 0.1, min_lr = 0.000001)\n",
    "\n",
    "# log_dir = f\"/home/tasos/logs/{int(time.time())}\"\n",
    "\n",
    "# tensorboard = TensorBoard(log_dir=log_dir)\n",
    "# callbacks = [early_stop, lr_redux, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "44/44 [==============================] - 5s 102ms/step - loss: 1.1381 - accuracy: 0.6215 - val_loss: 1.7115 - val_accuracy: 0.2222\n",
      "Epoch 2/50\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 0.1362 - accuracy: 0.9521 - val_loss: 2.1666 - val_accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 0.1458 - accuracy: 0.9624 - val_loss: 2.3815 - val_accuracy: 0.4722\n",
      "Epoch 4/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.0462 - accuracy: 0.9915 - val_loss: 2.3344 - val_accuracy: 0.2500\n",
      "Epoch 5/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0864 - accuracy: 0.9686 - val_loss: 2.2371 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 0.0286 - accuracy: 0.9941 - val_loss: 1.8483 - val_accuracy: 0.6111\n",
      "Epoch 7/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.0370 - accuracy: 0.9846 - val_loss: 1.7863 - val_accuracy: 0.4722\n",
      "Epoch 8/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0215 - accuracy: 0.9987 - val_loss: 1.7556 - val_accuracy: 0.5556\n",
      "Epoch 9/50\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 0.0313 - accuracy: 0.9866 - val_loss: 0.5053 - val_accuracy: 0.7778\n",
      "Epoch 10/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.2079 - val_accuracy: 0.8889\n",
      "Epoch 11/50\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.1237 - val_accuracy: 0.9444\n",
      "Epoch 13/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0212 - accuracy: 0.9986 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.0123 - accuracy: 0.9936 - val_loss: 2.3276e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.6030e-05 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.0299 - accuracy: 0.9863 - val_loss: 0.0607 - val_accuracy: 0.9722\n",
      "Epoch 18/50\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 0.0158 - accuracy: 0.9968 - val_loss: 1.8419e-05 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.1330 - accuracy: 0.9717 - val_loss: 0.0574 - val_accuracy: 0.9722\n",
      "Epoch 20/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.0255 - accuracy: 0.9906 - val_loss: 3.2739e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.0162 - accuracy: 0.9965 - val_loss: 3.7185e-06 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "44/44 [==============================] - 4s 81ms/step - loss: 0.0597 - accuracy: 0.9804 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 0.0637 - accuracy: 0.9783 - val_loss: 1.6018e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "44/44 [==============================] - 4s 81ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.5762e-06 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "44/44 [==============================] - 4s 80ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.5398e-06 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.0897e-05 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.3047e-06 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.0121 - accuracy: 0.9944 - val_loss: 4.3306e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "44/44 [==============================] - 4s 81ms/step - loss: 0.0193 - accuracy: 0.9868 - val_loss: 3.1478e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 0.1832 - accuracy: 0.9464 - val_loss: 1.8117 - val_accuracy: 0.7222\n",
      "Epoch 31/50\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 0.0680 - accuracy: 0.9771 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 9.9345e-05 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0489 - accuracy: 0.9858 - val_loss: 1.6987e-06 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0410 - accuracy: 0.9828 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 0.0138 - accuracy: 0.9944 - val_loss: 3.0605e-05 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.1047 - accuracy: 0.9767 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0460 - accuracy: 0.9826 - val_loss: 1.5213 - val_accuracy: 0.7500\n",
      "Epoch 38/50\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 0.0149 - accuracy: 0.9979 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 0.0154 - accuracy: 0.9946 - val_loss: 5.6221e-06 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 1.9405e-06 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.1948 - accuracy: 0.9572 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 0.0051 - accuracy: 0.9998 - val_loss: 4.1392e-07 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0130 - accuracy: 0.9932 - val_loss: 2.4975 - val_accuracy: 0.8333\n",
      "Epoch 44/50\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 0.2026 - accuracy: 0.9700 - val_loss: 0.2386 - val_accuracy: 0.9444\n",
      "Epoch 45/50\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 0.0641 - accuracy: 0.9866 - val_loss: 0.3373 - val_accuracy: 0.9444\n",
      "Epoch 46/50\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 0.0302 - accuracy: 0.9945 - val_loss: 1.6487e-05 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0017 - accuracy: 0.9990 - val_loss: 2.5166e-07 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.0174 - accuracy: 0.9963 - val_loss: 1.1491 - val_accuracy: 0.8333\n",
      "Epoch 49/50\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 0.1155 - accuracy: 0.9850 - val_loss: 2.3762e-05 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 0.0031 - accuracy: 0.9969 - val_loss: 3.7317e-06 - val_accuracy: 1.0000\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 1.5457e-06 - accuracy: 1.0000\n",
      "Test accuracy: 1.0\n",
      "Test loss: 1.5456788560186396e-06\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator\n",
    "    #callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "\n",
    "print(\"Test accuracy: {}\" .format(test_acc))\n",
    "print(\"Test loss: {}\" .format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model-shapes.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\Anaconda\\lib\\site-packages\\tensorflowjs\\converters\\keras_h5_conversion.py:123: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "tfjs_target_dir = \"C:/Users/younis3/Desktop/V6/Model/savedmodels/shapes\"\n",
    "\n",
    "model.save('model-shapes.model', tfjs_target_dir)\n",
    "tfjs.converters.save_keras_model(model, tfjs_target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/younis3/Desktop/V6/Model/train/train_fruits/seg_train/dog/dog-!-8.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-24d9ac42329d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/younis3/Desktop/V6/Model/train/train_fruits/seg_train/dog/dog-!-8.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIMG_HEIGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCHANNELS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Apps\\Anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2890\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2891\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2892\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/younis3/Desktop/V6/Model/train/train_fruits/seg_train/dog/dog-!-8.jpg'"
     ]
    }
   ],
   "source": [
    "test_images = []\n",
    "img_path = 'C:/Users/younis3/Desktop/V6/Model/train/train_fruits/seg_train/dog/dog-!-8.jpg'\n",
    "img = np.array(PIL.Image.open(img_path))\n",
    "img = np.resize(img, (IMG_HEIGHT, IMG_WIDTH, CHANNELS))\n",
    "img = img.astype('float32')\n",
    "img /= 255\n",
    "\n",
    "test_images.append(img)\n",
    "test_images = np.array(test_images)\n",
    "#plt.imshow(test_images)\n",
    "\n",
    "model.predict(test_images)\n",
    "model.predict_classes(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "#    def plot_image(i, predictions_array, true_label, img):\n",
    "#     true_label, img = true_label[i], img[i]\n",
    "#     plt.grid(False)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "\n",
    "#     plt.imshow(img)\n",
    "\n",
    "#     predicted_label = np.argmax(predictions_array)\n",
    "#     if predicted_label == true_label:\n",
    "#         color = 'blue'\n",
    "#     else:\n",
    "#         color = 'red'\n",
    "\n",
    "#     plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "#                             100*np.max(predictions_array),\n",
    "#                             class_names[true_label]),\n",
    "#                             color=color)\n",
    "\n",
    "# def plot_value_array(i, predictions_array, true_label):\n",
    "#     true_label = true_label[i]\n",
    "#     plt.grid(False)\n",
    "#     plt.xticks(range(6))\n",
    "#     plt.yticks([])\n",
    "#     thisplot = plt.bar(range(6), predictions_array, color=\"#777777\")\n",
    "#     plt.ylim([0, 1])\n",
    "#     predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "#     thisplot[predicted_label].set_color('red')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
